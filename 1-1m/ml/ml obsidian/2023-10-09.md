Variabili congiunte:
- aspettazione: aspettazione delle componenti
- varianza: diventa COVARIANCE MATRIX (componenti: covarianza tra due componenti)
  $\sigma_{X_i,X_j} = \textbf{Cov}(X_i,X_j) = E[(X_i-m_{X_i})(X_j-m_{X_j})]$
  (su diagonale: varianze di componenti)

Variabili indipendenti hanno covarianza nulla -> contrario non vale

DISUGUAGLIANZA DI CHEBYSHEV
$P[|X-\mu|>\epsilon] \leq \frac{\sigma^2}{\epsilon^2}$
con X r.v., $E[X]=\mu$, $Var[X]=\sigma^2$

Output del learner: $h_S : X \rightarrow Y$
	notazione per tenere conto di training set
Obbiettivo di learner: $h_S \approx f$ -> minimizzare generalization error $L_{D,f}(h)$ -> non conosco D e f -> consideriamo errore su training data: TRAINING ERROR / EMPIRICAL ERROR / EMPIRICAL RISK
$$L_S(h) = \frac{|\{ i:h(x_i)\neq y_i, 1 \leq i \leq m \}|}{m}$$
Numeratore: numero di istanze per cui prevediamo sbagliato
EMPIRICAL RISK MINIMIZATION (ERM): procedura per produrre in output $h$ che minimizza $L_S(h)$
Ha senso perchè abbiamo supposto che esistano $D$ e $f$
Però, si rischia overfitting -> potremmo memorizzare dati
	![[Pasted_image_20231009115242.png]]
	(quadri: piaciuto, cerchi: non piaciuto)
	supponiamo $D$ e $f$ tali che:
	- $D$: distr. uniforme in quadrato
	- label 1 se x in quadrato dentro, 0 altrimenti
	- area quadrato dentro = 1, area quadrato totale = 2
	Consideriamo classifier $h_S(x) = y_i$ se x in training set, 0 altrimenti
	Training error = 0 -> dà sempre label giusta -> non va bene
	Si sbaglia per ogni x in quadrato piccolo -> cattivo generalization error: OVERFITTING