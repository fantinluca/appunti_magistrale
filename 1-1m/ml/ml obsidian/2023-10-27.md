SGD (continuazione):
	algoritmo:
		$\vec{w} \leftarrow \vec{0}$;
		for $t\leftarrow0$ to $T-1$ do:
			prendo $i$ uniformemente a caso da $\{1,...,m\}$;
			$\vec{w}^{(t+1)} \leftarrow \vec{w}^{(t)} - \eta \nabla l(\vec{w}^{(t)}, (\vec{x}_i,y_i))$;
		return $\overline{w} = (\sum_{i=1}^T\vec{w}^{(t)})/T$
	Quanto vale gradiente di loss?
		calcoliamo $\nabla l(\vec{w}, (\vec{x}_i,y_i))$: $$\nabla l(\vec{w}, (\vec{x}_i,y_i)) = \begin{cases} \vec{0} & \mbox{if } y_i \langle \vec{w},\vec{x}_i \rangle > 0 \mbox{ (se istanza è classificata correttamente)} \\ \nabla(-y_i \langle \vec{w},\vec{x}_i \rangle) & \mbox{otherwise} \end{cases}$$supponiamo che $y_i \langle \vec{w},\vec{x}_i \rangle < 0$: $$\nabla(-y_i \langle \vec{w},\vec{x}_i \rangle) = \left[ \frac{\partial(-y_i \langle \vec{w},\vec{x}_i \rangle)}{\partial w_1}, ...,  \frac{\partial(-y_i \langle \vec{w},\vec{x}_i \rangle)}{\partial w_d}\right]^T$$chiamiamo: $$\vec{x}_i = \begin{bmatrix} x_{i1} \\ . \\ . \\ . \\ x_{id}\end{bmatrix}; \quad -y_i \langle \vec{w},\vec{x}_i \rangle = -y_i \sum_{j=1}^d(w_jx_{ij}) \quad \Longrightarrow \quad  \frac{\partial (-y_i, \langle \vec{w},\vec{x}_i \rangle)}{\partial w_j} = -y_ix_{ij}$$$$\nabla l (\vec{w}, \langle \vec{w},\vec{x}_i \rangle) =[-y_ix_{i1},...,-y_ix_{id}]^T = -y_i\vec{x}$$quindi, in pseudocodice, $\vec{w}^{(t+1)} \leftarrow \vec{w}^{(t)} - \eta \nabla l(\vec{w}^{(t)}, (\vec{x}_i,y_i))$ diventa: $$\mbox{if } y_i\langle\vec{w},\vec{x}_i\rangle<0\mbox{ then: }\vec{w}^{(t+1)} \leftarrow \vec{w}^{(t)} + \eta \cdot y_i\vec{x}_i$$
	Ora possiamo confrontare perceptron vs. SGD perceptron:
	- si sceglie punto classificato male vs. si sceglie punto a caso -> differenza principale
	- $\eta=1$ vs. $\eta$ parametro, si può scegliere valore
	- si ritorna migliore modello trovato (con minimo training error) vs. si ritorna media di modelli
	 In SGD perceptron si aggiorna modello se punto classificato male -> si può velocizzare algoritmo se, ad ogni iterazione, si sceglie punto classificato male a caso -> non c'è vera differenza tra algoritmi: SGD perceptron $\equiv$ perceptron

LINEAR REGRESSION (linear regression $\neq$ regression):
$\mathbb{X} = \mathbb{R}^d, \mathbb{Y} = \mathbb{R}$
Classe di ipotesi: $$\mathbb{H}_{reg}=L_d=\{\textbf{x}\rightarrow\langle\textbf{w},\textbf{x}\rangle+b\ |\ w\in\mathbb{R}^d,b\in\mathbb{R}\}$$(parametri: $\textbf{w},b$), $h\in\mathbb{H}_{reg}:\mathbb{R}^d\rightarrow\mathbb{R}$
Qual è loss function? Possiamo scegliere -> scelta comune: SQUARED LOSS: $$l(h,(\textbf{x},y)):=(h(\textbf{x})-y)^2$$Useremo ERM per linear regression e squared loss -> MEAN SQUARED ERROR: $$L_S(h)=\frac{1}{m} \sum_{i=1}^m(h(\textbf{x}_i)-y_i)^2$$Come trovare ipotesi ERM? LEAST SQUARES ALGORITHM: ipotesi migliore: $$\mbox{argmin}_{\textbf{w}}L_S(h_{\textbf{w}})=\mbox{argmin}_{\textbf{w}}\frac{1}{m}\sum_{i=1}^m(\langle \textbf{w}, \textbf{x}_i\rangle-y_i)^2 = \mbox{argmin}_{\textbf{w}}\sum_{i=1}^m(\langle \textbf{w}, \textbf{x}_i\rangle-y_i)^2$$Ultima formula: RESIDUAL SUM OF SQUARES (RSS)
Rappresentazione matriciale con DESIGN MATRIX $\textbf{X}$: $$\textbf{X} = \begin{bmatrix} ... \textbf{x}_1... \\ ... \ . \ ... \\ ... \ . \ ...\\ ... \ . \ ... \\ ... \textbf{x}_m ... \end{bmatrix}; \quad \textbf{y} = \begin{bmatrix} y_1 \\ . \\ .\\ . \\ y_m \end{bmatrix}\quad \Longrightarrow \quad \sum_{i=1}^m (\langle \textbf{w},\textbf{x}_i\rangle-y_i)^2 = (\textbf{y}-\textbf{Xw})^T(\textbf{y}-\textbf{Xw})$$Vogliamo ipotesi che minimizzi: $$\mbox{argmin}_{\textbf{w}}RSS(\textbf{w}) = \mbox{argmin}_{\textbf{w}}(\textbf{y}-\textbf{Xw})^T(\textbf{y}-\textbf{Xw})$$Coefficiente di grado 2: $\textbf{X}$: positivo -> abbiamo funzione convessa -> calcoliamo derivata/gradiente rispetto a $\textbf{w}$ e mettendolo a 0: $$\frac{\partial RSS(\textbf{w})}{\partial\textbf{w}} = -2\textbf{X}^T(\textbf{y}-\textbf{Xw})=0$$$$-2\textbf{X}^T(\textbf{y}-\textbf{Xw})=0 \rightarrow -2\textbf{X}^T\textbf{y}+2\textbf{X}^T\textbf{Xw}=0 \rightarrow \textbf{X}^T\textbf{Xw}=\textbf{X}^T\textbf{y} \rightarrow \textbf{w}=(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{y}$$$(\textbf{X}^T\textbf{X})$ dev'essere invertibile:
- se lo è, abbiamo soluzione ERM
  In calcolo di $(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{y}$, operazione più costosa è inversione di matrice $(d+1)\times(d+1)$ -> buono, si può fare nella pratica (si vuole $d<<m$)
- se non lo è:
	$\textbf{A}=\textbf{X}^T\textbf{X}$ -> definiamo $\textbf{A}^+$ GENERALIZED INVERSE: $\textbf{A}\textbf{A}^+\textbf{A}=\textbf{A}$
	Prop: se $\textbf{A}=\textbf{X}^T\textbf{X}$ non invertibile, allora $\hat{w} = \textbf{A}^+\textbf{X}^T\textbf{y}$ è soluzione a $\textbf{X}^T\textbf{Xw}=\textbf{X}^T\textbf{y}$
	$\textbf{A}$ è simmetrica -> decomposizione con autovalori (valore $\lambda$ tale che $\textbf{A}\vec{x}=\lambda\vec{x}$ ($\vec{x}$: autovettore)): $\textbf{A}=\textbf{V}\textbf{D}\textbf{V}^T$, con:
	- $\textbf{D}$: diagonale con autovalori di $\textbf{A}$
	- $\textbf{V}$: matrice ortonormale ($\textbf{V}^T\textbf{V} = \textbf{I}_{d\times d}$)
	Definiamo $\textbf{D}^+$ matrice diagonale tale che: $$\textbf{D}_{i,j}^+ = \begin{cases} 0 & \mbox{if } \textbf{D}_{i,j}=0 \\ \frac{1}{\textbf{D}_{i,j}} & \mbox{otherwise}\end{cases}$$Se calcolo $\textbf{D}\textbf{D}^+$, matrice identità in angolo alto sx circondata da 0
	$\textbf{A}^+=\textbf{V}\textbf{D}^+\textbf{V}^T$ è generalized inverse di $\textbf{A}$?$$\textbf{A}\textbf{A}^+\textbf{A}=\textbf{V}\textbf{D}\textbf{V}^T\textbf{V}\textbf{D}^+\textbf{V}^T\textbf{V}\textbf{D}\textbf{V}^T$$Dato che $\textbf{V}$ è ortonormale: $$=\textbf{V}\textbf{D}\textbf{D}^+\textbf{D}\textbf{V}^T=\textbf{V}\textbf{D}\textbf{V}^T=\textbf{A}$$In pratica is usa MOORE-PENROSE GENERALIZED INVERSE