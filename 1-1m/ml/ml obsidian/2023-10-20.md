Classificazione lineare: vogliamo trovare buona ipotesi (supponiamo insieme di ipotesi = semispazi) -> buon generalization error, nella pratica minimizziamo training error (ERM)
Non posso provare tutte ipotesi -> usiamo PERCEPTRON ALGORITHM (classificazione lineare con 0-1 loss)
Consideriamo training set $S = \{ (\textbf{x}_i,y_i) \ | \ i \in [1,m] \}$
LINEARLY SEPARABLE DATA: esiste modello $\textbf{w}$ tale che $y_i \langle \textbf{w}, \textbf{x}_i \rangle > 0 \ \forall i=1,...,m$ -> modello $\textbf{w}$ ha predetto label giuste ($h_{\textbf{w}}(\textbf{x}_i) = y_i$) -> realizability assumption per training set

PERCEPTRON
Input: training set
	initialize $\textbf{w}^{(1)} = (0,...,0)$
	for $t=1,2,...$ do
		if $\exists i | y_i \langle \textbf{w}^{(t)}, \textbf{x}_i \rangle \leq 0$ then $\textbf{w}^{(t+1)} \leftarrow \textbf{w}^{(t)} + y_i\textbf{x}_i$;
		else return $\textbf{w}^{(t)}$
$\textbf{w}^{(t)}$ ritornato quando abbiamo linearly separable data
Update a $\textbf{w}^{(t)}$ fatto sperando che ora sia giusto
Terminazione algoritmo dipende da realizability assumption, ma non in modo ovvio
Prop:
	dati linearly separable data, siano:
	- $B = min \{ ||\textbf{w}|| \ | \ y_i \langle \textbf{w}, \textbf{x}_i \rangle \geq 1 \ \forall i=1,...,m \}$
	- $R = max_i ||\textbf{x}_i||$
	algoritmo finisce dopo al massimo $(RB)^2$
Esistono tanti modelli per linearly separable data -> SVM: prende modello più lontano da punti, no garanzie per perceptron

Proprietà:
- facile da implementare
- per dati separabili:
	- terminazione garantita
	- potrebbe richiedere # it. esponenziale in d
	- potenzialmente soluzioni multiple, la scelta dipende da valori iniziali
- per dati non separabili: lo faccio andare per numero massimo di iterazioni e ritorno modello con errore minimo prodotto da varie iterazioni -> pocket algorithm

Nuova interpretazione di perceptron: GRADIENT DESCENT
Risolviamo:
- binary classification problem
- con modelli lineari
- loss $l(\textbf{w}, (\textbf{x},y)) = max \{ 0, -y \langle \textbf{w},\textbf{x} \rangle \}$ (simile a 0-1 loss)
Troviamo ipotesi con ERM -> STOCHASTIC GRADIENT DESCENT

GRADIENT DESCENT: approccio generale per minimizzare funzione convessa differenziabile $f(\textbf{w})$ -> se ho algoritmi che mi danno valori di funzione e di derivata in punto, seguo gradiente negativo fino a trovare minimo -> non serve minimo globale: ci va bene così
Poniamo training error come $f(\textbf{w})$ (in formula, $m,\textbf{x}_i,y_i$ sono fissi)
	consideriamo $f: \mathbb{R}^d \rightarrow \mathbb{R}$ -> gradiente di $f$ punta verso massimo tasso di crescita intorno a $\textbf{w}$
	sia $\eta \in \mathbb{R}_+$: LEARNING PARAMETER (spesso si usa parametro dipendente dal tempo)
	Algoritmo:
		$\textbf{w}^{(0)} \leftarrow \textbf{0}$;
		for $t \leftarrow 0$ to $T-1$ do
			$\textbf{w}^{(t+1)} \leftarrow \textbf{w}^{(t)} - \eta \nabla f(\textbf{w}^{t})$
		return $\bar{\textbf{w}} = \frac{1}{T} \sum_{t-1}^T \textbf{w}^{(t)}$
Output potrebbe essere anche $\textbf{w}^{(T)}$ oppure $\mbox{argmin}_{\textbf{w}^{(t)} \in \{ 1,...,T \}} f(\textbf{w}^{(t)})$
Ritornare $\bar{\textbf{w}}$ utile per funzioni nondifferenziabili e per SGD
Ci sono garanzie su it. necessaria a GD per ritornare buon valore di $\bar{\textbf{w}}$ con certe supposizioni su $f$

STOCHASTIC GRADIENT DESCENT: ci si muove in direzione casuale che in media è quella giusta -> usiamo vettore casuale con aspettazione uguale a gradiente
		$\textbf{w}^{(0)} \leftarrow \textbf{0}$;
		for $t \leftarrow 0$ to $T-1$ do
			scelgo $\textbf{v}_t$ a caso da distribuzione tale che $\textbf{E}[\textbf{v}_t|\textbf{w}^{(t)}] \in \nabla f(\textbf{w}^{(t)})$
			$\textbf{w}^{(t+1)} \leftarrow \textbf{w}^{(t)} - \eta \textbf{v}_t$
		return $\bar{\textbf{w}} = \frac{1}{T} \sum_{t-1}^T \textbf{w}^{(t)}$
Anche qui garanzie su numero iterazioni come GD

Usiamo GD per minimizzare training error -> $f(\textbf{w}) = L_S(\textbf{w})$ -> gradiente dipende da tutti i punti -> computazione potrebbe richiedere molto tempo
Se uso SGD, come scelgo $\textbf{v}_t$ adatto? -> posso dimostrare che se prendo punto random da S e scelgo $\textbf{v}_t \in \nabla l(\textbf{w}^{(t)}, (\textbf{x}_i,y_i))$, soddisfa requisiti e devo calcolare solo una loss

Tornando a problema di classificazione, come trovo soluzione ERM? Con SGD
	SGD: prendo a caso $i \in \{ 1,...,m \}$
	Sia $(\textbf{x}',\textbf{y}')$ istanza corrispondente -> prendiamo vettore $\nabla l(\textbf{w},(\textbf{x}',y'))$ -> proviamo sua aspettazione = gradiente loss: $\mathbb{E}[\nabla l(\textbf{w}, (\textbf{x}',y'))] = \sum_{i=1}^m \mathbb{Pr} [(\textbf{x}',y') = (\textbf{x}_i,y_i)] \cdot \nabla l(\textbf{w},(\textbf{w}_i,y_i)) = \frac{1}{m} \sum_{i=1}^m \nabla l(\textbf{w}, (\textbf{x}_i,y_i)) = \nabla L_S(\textbf{w})$