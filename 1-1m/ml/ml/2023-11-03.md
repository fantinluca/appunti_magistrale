Supponendo $\mathbb{X}=\mathbb{R}$, ci interessa usare modelli polinomiali (grado $r$): $\sum_{i=0}^rw_ix^i$ -> assomiglia a modelli lineare -> dato $x\in\mathbb{R}$, calcoliamo vettore: $$\textbf{x}' = \begin{bmatrix}1\\x\\x^2\\\vdots\\x^r\end{bmatrix}, \quad \textbf{w}=\begin{bmatrix}w_0\\w_1\\w_2\\\vdots\\w_r\end{bmatrix} \quad \Rightarrow \quad <\textbf{w},\textbf{x}>=\sum_{i=0}^rw_ix^i$$La classe di ipotesi di modelli lineare per $\textbf{x}$ corrisponde a classe di ipotesi polinomiali di grado $r$ per $x$
Linear regression -> funzione lineare di parametri -> si può usare perceptron, SGD, etc.

Supponiamo $\mathbb{X}=\mathbb{R}^d$ -> usiamo FEATURE EXPANSION: costruire nuovo vettore: $$\textbf{x}=\begin{bmatrix}x_1\\\vdots\\x_d\end{bmatrix} \quad \Longrightarrow \quad \textbf{x}'=[1,x_1,...,x_d,x_1^2,...x_d^2,...,x_1^r,...,x_d^r]^T$$Ora possiamo usare modello lineare per $\mathbb{x}'$
Supponiamo $d=2$: $\textbf{x}=[x_1,x_2]$ -> a espansione normale potrei aggiungere termini come $w_5x_1x_2$ -> feature expansion diversa: $$\textbf{x}\in\mathbb{R}^3, \textbf{x}=\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}, r=2 \quad \Rightarrow \quad \textbf{x}=[1,x_1,x_2,x_3,x_1^2,x_2^2,x_3^2,x_1x_2,x_1x_3,x_2x_3]$$Poi uso modelli lineari con $\textbf{x}$

FEATURE NORMALIZATION
Normalizzare feature in modo tale che:
- media di ogni feature su training set sia 0
- deviazione standard di ogni feature su training set sia 1
Questo è passaggio di data normalization importante per:
- stabilità di computazione
- interpretabilità di modelli lineari (alto peso -> feature importante)
Se costruiamo modello usando dati normalizzati, la stessa funzione di normalizzazione dev'essere applicata a dati in input

Soluzione es 2:
	consideriamo ipotesi: $h(\textbf{x}) =l \ \forall \textbf{x}\in\mathbb{X}$ -> generalization error: $$L_{\mathbb{D}}(h)=\mathbb{E}_{(\textbf{x},y)\sim\mathbb{D}}[l(h,(\textbf{x},y))]=0\cdot\mathbb{Pr}[l(h,(\textbf{x},y))=0]+1\cdot\mathbb{Pr}[l(h,(\textbf{x},y))=1]=$$$$=\mathbb{Pr}_{(\mathbf{x},y)\sim\mathbb{D}}[l(h,(\textbf{x},y))=1]=\mathbb{Pr}_{(\mathbf{x},y)\sim\mathbb{D}}[h(\mathbf{x})\neq y]=\mathbb{Pr}_{(\mathbf{x},y)\sim\mathbb{D}}[l\neq y]=1-\mathbb{Pr}_{(\mathbf{x},y)\sim\mathbb{D}}[h(\mathbf{x})= y]=$$$$=1-p_l$$Da questo risultato, ipotesi $h(\textbf{x})=1 \ \forall\textbf{x}\in\mathbb{X}$ ha generalization error 0.05 -> modello stupido ha generalization error uguale a algoritmo di esercizio -> questo implica che algoritmo di esercizio non ha imparato relazione significativa tra features e label, ma ha imparato che label 1 appare quasi sempre con distribuzione (questa probabilità è facile da imparare con abbastanza dati) -> algoritmo di esercizio non ha imparato nulla di utile in termini di ML

Soluzione es 3:
	sappiamo che $L_S(h_S)=0$ -> da dimostrazione di corollario 1.3 sappiamo $\mathbb{Pr}[L_{\mathbb{D}}[h_S]>\epsilon]\leq|\mathbb{H}|e^{-\epsilon m}$
	Dato $\delta$, fissiamo $\epsilon=\frac{1}{m}ln(\frac{|\mathbb{H}|}{\delta})$ -> $\mathbb{Pr}[L_{\mathbb{D}}h_S]\leq\frac{1}{m}ln(\frac{|\mathbb{H}|}{\delta})]\geq1-\delta$
	Scegliamo $\delta=0.1$ -> con probabilità $\geq0.9$: $$L_{\mathbb{D}}(h_S)\leq\frac{1}{4}ln(\frac{4}{0.1})\approx0.75$$Grande errore -> causato da basso numero di sample

Soluzione es 1:
	dato $h_a\in\mathbb{H}$, training error per questa ipotesi: $$L_S(h_S)=\frac{1}{m}\sum_{i=1}^m(h_a(\textbf{x})-y_i)^2=\frac{1}{m}\sum_{i=1}^m(a-y_i)^2$$Trovare ipotesi che minimizza training error vuol dire trovare $a$ che minimizza $$L_S(h_S)=\frac{1}{m}\sum_{i=1}^m(a-y_i)^2=f(a) = (...)a^2+(...)a+(...)$$Funzione convessa di $a$ (coefficiente di $a^2$ sempre positivo) sempre positiva -> troviamo $a^*$ tale che: $$\frac{dL_s(h_a)}{da}=0 \quad \Rightarrow \quad \frac{d}{da}\left(\frac{1}{m}\sum_{i=1}^m(a-y_i)^2\right)=\frac{1}{m}\sum_{i=1}^m\left(\frac{d}{da}(a-y_i)^2\right)=\frac{1}{m}\sum_{i=1}^m2(a-y_i)=$$$$=\frac{2}{m}\sum_{i=1}^m(a-y_i)=0 \quad \Rightarrow \quad \sum_{i=1}^m(a-y_i)=0 \quad \Rightarrow \quad \sum_{i=1}^ma-\sum_{i=1}^my_i=0 \quad \Rightarrow \quad \sum_{i=1}^ma=\sum_{i=1}^my_i$$$$\quad \Rightarrow \quad ma=\sum_{i=1}^my_i \quad \Rightarrow \quad a=\frac{\sum_{i=1}^my_i}{m}=\overline{y}$$
	$R^2$ contiene errore di $\hat{h}$ relativo a errore di miglior modello stupido -> $R^2$ è misura di quanto bene $\hat{h}$ funziona rispetta a miglior modello stupido -> $R^2 \in (-\infty,1]$