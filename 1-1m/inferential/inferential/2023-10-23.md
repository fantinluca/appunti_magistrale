MULTIVARIATE DELTA METHOD:
	$X_n=(X_{n1},...,X_{np})$ sequenza di rve tali che $$\sqrt{n}(X_n-\mu) \xrightarrow{d} N_p(0,\Sigma)$$$g(x)=(g_1(x),...,g_k(x))$ con $g:\mathbb{R}^p \rightarrow \mathbb{R}^k, k\leq p$ tale che matrice di derivate parziali sia continua e non vanisca in vicino di $\mu$ (dev'essere funzione smooth di $\mu$): $$B(x)=[b_{ij}]=\left[\frac{\partial g_i(x)}{\partial x_j}\right], x=(x_1,...,x_p)$$Dato $B_{\mu}=B(\mu)$: $$\sqrt{n}(g(X_n)-g(\mu)) \xrightarrow{d} N_k(0,B_{\mu}\Sigma B_{\mu}^T)$$

### DESCRIPTIVE STATISTICS AND STATISTICAL MODELS
Argomento: convertire problemi in modelli statistici
Nella pratica non osserviamo liste di rv/rve, ma osservazioni di rv -> $x_1,...,x_n$ OBSERVED SAMPLE, $n$ SAMPLE SIZE -> si suppone che observed sample sia osservazione di RANDOM SAMPLE $X_1,...,X_n$ ($X_i$ indipendenti tra loro, stessa densità marginale $f$)
Indipendenti -> densità congiunta: $$f(x_1,...,x_n;\theta) = \Pi_{i=0}^n f(x_i;\theta)$$(densità di $X_i$ dipendente da parametro sconosciuto $\theta$)

Di solito vogliamo qualche funzione di sample: DESCRIPTIVE/SUMMARY STATISTICS -> possono valere per random e observed sample -> esempi di MOMENT-BASED STATISTICS:
- sample average $\overline{X}=\frac{1}{n} \sum_i X_i$ ($\overline{x}=\frac{1}{n} \sum_i x_i$)
- sample variance $S^2 = \frac{1}{n-1} \sum_i (X_i-\overline{X})^2$ ($s^2 = \frac{1}{n-1} \sum_i (x_i-\overline{x})^2$ -> s: STANDARD DEVIATION)
- sample $k$th moment $\overline{X^k} = \frac{1}{n} \sum_i X_i^k$ ($\overline{x^k} = \frac{1}{n} \sum_i x_i^k$)

Moments: via più comune per riassumere dati -> altro metodo: mettere in ordine
$X_{(1)} = \text{min}_{i\in[1,n]} X_i$ osservazione più piccola, seguita da $X_{(2)}, X_{(3)}, ...$ fino a $X_{(n)} = \text{max}_{i\in[1,n]}$ -> lista $X_{(1)},...,X_{(n)}$: ORDER STATISTICS -> summary statistics basate su order statistics (queste sono versioni per random sample) ($[x]$ restituisce intero maggiore $\leq x$):
- mediana: $$Q_2 = \begin{cases} X_{\left(\frac{n+1}{2}\right)} & \mbox{if } n \mbox{ is even} \\ (X_{\left(\frac{n}{2}\right)} + X_{\left(\frac{n}{2}+1\right)})/2 & \mbox{if } n \mbox{ is odd} \end{cases}$$
- primo e terzo quantile: $$Q_1=X_{[0.25(n+1)]}, Q_3=X_{[0.75(n+1)]}$$
- $p$th sample quantile ($p\in(p+1)$): $$X_{[p(n+1)]}$$
- inter quartile range: $$IQR = Q_3-Q_1$$
- deviazione assoluta mediana da mediana: $$MAD=\text{median}(|X_1-Q_2|,...,|X_n-Q_2|)$$

- MISURE DI LOCATION: $\overline{X},Q_1,Q_2,Q_3,X_{[p(n+1)]}$, usate per indicare un singolo tipico valore di sample
- MISURE DI SPREAD: $S^2,S,MAD,IQR$, usate per descrivere variabilità di sample
- SKEWNESS: quanto asimmetrica è distribuzione
- KURTOSIS: quanto lunghe sono code di distribuzione

ISTOGRAMMA: utile per dare idea di densità di sample
	$x_1,...,x_n$ observed sample -> consideriamo partizione in intervalli $(a_{j-1},a_j], j\in[1,m], m<n$ di sample (non ci sono buchi tra intervalli)
	Definito da funzione definita a pezzi

EMPIRICAL DISTRIBUTION FUNCTION (EDF):
	random sample $X_1,...,X_n$: $$F_n(x) = \sum_{i=1}^n I_{X_i} (x) \ \forall x\in\mathbb{R}$$
	con $I_{X_i}(x)$ è rv Bernoulli con probabilità di successo $P(X_i\leq x)$ -> $F_n$ è quindi rv
	Versione con observed sample: $$\hat{F}_n(x) = n^{-1} \sum_{i=1}^n \textbf{1}_{X_i}(x) \ \forall x\in\mathbb{R}$$$\textbf{1}_{X_i}(x)$ ha valore 1 se $x_i\leq x$, 0 altrimenti
	Queste funzioni puntano alla distribuzione della popolazione $F(x)$ -> si avvicina sempre di più per $n$ grandi
Più adatta di istogramma -> proprietà migliori: approssimazione migliore, si avvicina molto più velocemente a vera distribuzione rispetto a istogramma