SUPERSCALAR architecture: processori con molto parallelismo, ma non trasparente; se ne occupa macchina
Bisogna rendere macchina parallela in modo esplicito
Inizio 2000: non ancora evoluzione ovvia -> consumo energetico non ancora considerato
Circa 2005: grosso processori iniziarono con mettere più processori su chip -> parallel computing started spreading

Big parallel multiprocessor machines
Best ones: paragraph on HPC TOP 500 -> ogni 6 mesi, viene fatta lista di migliori supercomputer
(PETAFLOP: 10^15 float point operations)
$R_{peak}$: massimo teorico, $R_{max}$: performance misurata su pacchetto/benchmark specifico (linpack)
Vera sfida: ridurre consumo energetico
Utilizzi di Top 500: molti c'entrano con fisica e ingegneria, ma anche scienze biologiche -> in ultimi anni, molte richieste da AI

Parallel computing ormai molto diffuso -> compagnia NVIDIA:
- fondata per acceleratori per grafiche 
- poi svilupparono architettura GPU -> specializzata, ma poi si rivelò utile per molti altri ambiti
- GPU si diffuse sempre più -> usate in molti dei Top 500
- architetture molto utili per ML/AI

Aree di principale utilizzo per HPC:
- FISICA:
	- QED: quantum field theory; elettricità e interazioni deboli

